"""Methods for saving and loading model responses."""

import abc
import csv
import json
import logging
import re
import sqlite3
from pathlib import Path

from prpl_llm_utils.structs import Query, Response


class ResponseNotFound(Exception):
    """Raised during cache lookup if a response is not found."""


class PretrainedLargeModelCache(abc.ABC):
    """Base class for model caches."""

    @abc.abstractmethod
    def try_load_response(self, query: Query, model_id: str) -> Response:
        """Load a response or raise ResponseNotFound."""

    @abc.abstractmethod
    def save(self, query: Query, model_id: str, response: Response) -> None:
        """Save the response for the query."""

    @abc.abstractmethod
    def try_load_responses(
        self, query: Query, model_id: str, num_responses: int
    ) -> list[Response | None]:
        """Load multiple responses for a query.

        Returns a list of length num_responses where each element is either:
        - A Response object if cached
        - None if not cached

        This allows partial cache hits where some responses are cached
        and others need to be queried.
        """

    @abc.abstractmethod
    def save_response_at_index(
        self, query: Query, model_id: str, response: Response, index: int
    ) -> None:
        """Save a response at a specific index for multi-response queries."""


class FilePretrainedLargeModelCache(PretrainedLargeModelCache):
    """A cache that saves and loads from individual files."""

    def __init__(self, cache_dir: Path) -> None:
        self._cache_dir = cache_dir
        self._cache_dir.mkdir(exist_ok=True)

    def _get_cache_dir_for_query(self, query: Query, model_id: str) -> Path:
        query_id = re.sub(r"[<>:\"/\\|?*']", "_", query.get_readable_id())
        cache_foldername = f"{model_id}_{query_id}"
        cache_folderpath = self._cache_dir / cache_foldername
        cache_folderpath.mkdir(exist_ok=True)
        return cache_folderpath

    def try_load_response(self, query: Query, model_id: str) -> Response:
        cache_dir = self._get_cache_dir_for_query(query, model_id)
        if not (cache_dir / "prompt.txt").exists():
            raise ResponseNotFound
        # Load the saved completions.
        completion_file = cache_dir / "completion.txt"
        with open(completion_file, "r", encoding="utf-8") as f:
            completion = f.read()
        # Load the metadata.
        metadata_file = cache_dir / "metadata.json"
        with open(metadata_file, "r", encoding="utf-8") as f:
            metadata = json.load(f)
        # Create the response.
        response = Response(completion, metadata)
        logging.debug(f"Loaded model response from {cache_dir}.")
        return response

    def save(self, query: Query, model_id: str, response: Response) -> None:
        cache_dir = self._get_cache_dir_for_query(query, model_id)
        # Cache the text prompt.
        prompt_file = cache_dir / "prompt.txt"
        with open(prompt_file, "w", encoding="utf-8") as f:
            f.write(query.prompt)
        # Cache the image prompt if it exists.
        if query.imgs is not None:
            imgs_folderpath = cache_dir / "imgs"
            imgs_folderpath.mkdir(exist_ok=True)
            for i, img in enumerate(query.imgs):
                filename_suffix = str(i) + ".jpg"
                # Convert RGBA to RGB if necessary (JPEG doesn't support transparency)
                if img.mode == "RGBA":
                    rgb_img = img.convert("RGB")
                    rgb_img.save(imgs_folderpath / filename_suffix)
                else:
                    img.save(imgs_folderpath / filename_suffix)
        # Cache the text response.
        completion_file = cache_dir / "completion.txt"
        with open(completion_file, "w", encoding="utf-8") as f:
            f.write(response.text)
        # Cache the metadata.
        metadata_file = cache_dir / "metadata.json"
        with open(metadata_file, "w", encoding="utf-8") as f:
            json.dump(response.metadata, f)
        logging.debug(f"Saved model response to {cache_dir}.")

    def try_load_responses(
        self, query: Query, model_id: str, num_responses: int
    ) -> list[Response | None]:
        """Load multiple responses for a query."""
        cache_dir = self._get_cache_dir_for_query(query, model_id)
        if not (cache_dir / "prompt.txt").exists():
            return [None] * num_responses

        responses: list[Response | None] = []
        for i in range(num_responses):
            completion_file = cache_dir / f"completion_{i}.txt"
            metadata_file = cache_dir / f"metadata_{i}.json"

            if completion_file.exists() and metadata_file.exists():
                with open(completion_file, "r", encoding="utf-8") as f:
                    completion = f.read()
                with open(metadata_file, "r", encoding="utf-8") as f:
                    metadata = json.load(f)
                responses.append(Response(completion, metadata))
                logging.debug(f"Loaded response {i} from {cache_dir}.")
            else:
                responses.append(None)

        return responses

    def save_response_at_index(
        self, query: Query, model_id: str, response: Response, index: int
    ) -> None:
        """Save a response at a specific index."""
        cache_dir = self._get_cache_dir_for_query(query, model_id)

        # Cache the text prompt if not already cached.
        prompt_file = cache_dir / "prompt.txt"
        if not prompt_file.exists():
            with open(prompt_file, "w", encoding="utf-8") as f:
                f.write(query.prompt)

        # Cache the image prompt if it exists and not already cached.
        if query.imgs is not None:
            imgs_folderpath = cache_dir / "imgs"
            if not imgs_folderpath.exists():
                imgs_folderpath.mkdir(exist_ok=True)
                for i, img in enumerate(query.imgs):
                    filename_suffix = str(i) + ".jpg"
                    if img.mode == "RGBA":
                        rgb_img = img.convert("RGB")
                        rgb_img.save(imgs_folderpath / filename_suffix)
                    else:
                        img.save(imgs_folderpath / filename_suffix)

        # Cache the text response at the specified index.
        completion_file = cache_dir / f"completion_{index}.txt"
        with open(completion_file, "w", encoding="utf-8") as f:
            f.write(response.text)

        # Cache the metadata at the specified index.
        metadata_file = cache_dir / f"metadata_{index}.json"
        with open(metadata_file, "w", encoding="utf-8") as f:
            json.dump(response.metadata, f)

        logging.debug(f"Saved response {index} to {cache_dir}.")


class SQLite3PretrainedLargeModelCache(PretrainedLargeModelCache):
    """A cache that uses a SQLite3 database."""

    def __init__(self, database_path: Path) -> None:
        self._database_path = database_path
        self._database_path.parent.mkdir(exist_ok=True)
        self._initialized = False
        self._hyperparameter_keys: set[str] | None = None

    def _get_query_hash(self, query: Query, model_id: str) -> str:
        """Get a unique hash for the query and model combination."""
        return f"{model_id}_{hash(query)}"

    def _ensure_initialized(self, query: Query) -> None:
        """Initialize the database with the required tables and columns."""
        if self._initialized:
            # Verify hyperparameter keys are consistent.
            if query.hyperparameters is not None:
                current_keys = set(query.hyperparameters.keys())
                if self._hyperparameter_keys is not None:
                    assert current_keys == self._hyperparameter_keys, (
                        f"Hyperparameter changed from {self._hyperparameter_keys} "
                        f"to {current_keys}. All queries must use the same."
                    )
                else:
                    self._hyperparameter_keys = current_keys
            return

        with sqlite3.connect(self._database_path) as conn:
            # Create base table.
            conn.execute(
                """
                CREATE TABLE IF NOT EXISTS responses (
                    query_hash TEXT PRIMARY KEY,
                    model_id TEXT NOT NULL,
                    prompt TEXT NOT NULL,
                    images_hash TEXT,
                    completion TEXT NOT NULL,
                    metadata TEXT NOT NULL
                )
            """
            )

            # Add response_index column for multi-response support.
            # First check if it exists.
            cursor = conn.execute("PRAGMA table_info(responses)")
            columns = [info[1] for info in cursor.fetchall()]

            if "response_index" not in columns:
                # Add response_index column with default 0 for
                # backward compatibility.
                try:
                    conn.execute(
                        """ALTER TABLE responses ADD COLUMN
                           response_index INTEGER DEFAULT 0"""
                    )
                    # For proper multi-response support, we need a composite key.
                    # Since SQLite doesn't support modifying PRIMARY KEY,
                    # we'll create a new table and migrate data.
                    conn.execute(
                        """
                        CREATE TABLE IF NOT EXISTS responses_new (
                            query_hash TEXT NOT NULL,
                            response_index INTEGER NOT NULL DEFAULT 0,
                            model_id TEXT NOT NULL,
                            prompt TEXT NOT NULL,
                            images_hash TEXT,
                            completion TEXT NOT NULL,
                            metadata TEXT NOT NULL,
                            PRIMARY KEY (query_hash, response_index)
                        )
                        """
                    )
                    # Copy data from old table to new table.
                    conn.execute(
                        """
                        INSERT INTO responses_new
                        SELECT query_hash, response_index, model_id, prompt,
                               images_hash, completion, metadata
                        FROM responses
                        """
                    )
                    # Drop old table and rename new table.
                    conn.execute("DROP TABLE responses")
                    conn.execute("ALTER TABLE responses_new RENAME TO responses")
                except sqlite3.OperationalError:
                    # Migration already done or column exists.
                    pass

            # Add hyperparameter columns if present.
            if query.hyperparameters is not None:
                self._hyperparameter_keys = set(query.hyperparameters.keys())
                for key in self._hyperparameter_keys:
                    try:
                        conn.execute(f"ALTER TABLE responses ADD COLUMN {key} TEXT")
                    except sqlite3.OperationalError:
                        # Column already exists, ignore.
                        pass

            conn.commit()
            self._initialized = True

    def try_load_response(self, query: Query, model_id: str) -> Response:
        self._ensure_initialized(query)
        query_hash = self._get_query_hash(query, model_id)

        with sqlite3.connect(self._database_path) as conn:
            cursor = conn.execute(
                """SELECT completion, metadata FROM responses
                   WHERE query_hash = ? AND response_index = 0""",
                (query_hash,),
            )
            result = cursor.fetchone()

            if result is None:
                raise ResponseNotFound

            completion, metadata_json = result
            metadata = json.loads(metadata_json)
            response = Response(completion, metadata)
            logging.debug(
                f"Loaded model response from SQLite for query hash {query_hash}."
            )
            return response

    def save(self, query: Query, model_id: str, response: Response) -> None:
        self._ensure_initialized(query)
        self.save_response_at_index(query, model_id, response, index=0)

    def try_load_responses(
        self, query: Query, model_id: str, num_responses: int
    ) -> list[Response | None]:
        """Load multiple responses for a query."""
        self._ensure_initialized(query)
        query_hash = self._get_query_hash(query, model_id)

        responses: list[Response | None] = [None] * num_responses

        with sqlite3.connect(self._database_path) as conn:
            for i in range(num_responses):
                cursor = conn.execute(
                    """SELECT completion, metadata FROM responses
                       WHERE query_hash = ? AND response_index = ?""",
                    (query_hash, i),
                )
                result = cursor.fetchone()

                if result is not None:
                    completion, metadata_json = result
                    metadata = json.loads(metadata_json)
                    responses[i] = Response(completion, metadata)
                    logging.debug(
                        f"Loaded response {i} from SQLite for query hash {query_hash}."
                    )

        return responses

    def save_response_at_index(
        self, query: Query, model_id: str, response: Response, index: int
    ) -> None:
        """Save a response at a specific index."""
        self._ensure_initialized(query)
        query_hash = self._get_query_hash(query, model_id)

        # Prepare the data for storage.
        images_hash = None
        if query.imgs is not None:
            img_hash_list = query.robust_image_hash_list()
            images_hash = json.dumps(img_hash_list)

        metadata_json = json.dumps(response.metadata)

        # Build base columns and values.
        columns = [
            "query_hash",
            "response_index",
            "model_id",
            "prompt",
            "images_hash",
            "completion",
            "metadata",
        ]
        values = [
            query_hash,
            index,
            model_id,
            query.prompt,
            images_hash,
            response.text,
            metadata_json,
        ]

        # Add hyperparameters if present.
        if query.hyperparameters is not None:
            columns.extend(query.hyperparameters.keys())
            values.extend(json.dumps(value) for value in query.hyperparameters.values())

        placeholders = ["?"] * len(columns)
        sql = f"""
            INSERT OR REPLACE INTO responses
            ({', '.join(columns)})
            VALUES ({', '.join(placeholders)})
        """

        with sqlite3.connect(self._database_path) as conn:
            conn.execute(sql, values)
            conn.commit()

        logging.debug(
            f"Saved response {index} to SQLite database for query hash {query_hash}."
        )

    def to_csv(self, csv_path: Path) -> None:
        """Dump the entire responses table to a CSV file."""
        with sqlite3.connect(self._database_path) as conn:
            cursor = conn.cursor()

            # Fetch all columns
            cursor.execute("PRAGMA table_info(responses)")
            columns = [info[1] for info in cursor.fetchall()]

            cursor.execute("SELECT * FROM responses")
            rows = cursor.fetchall()

            # Write to CSV
            with open(csv_path, mode="w", newline="", encoding="utf-8") as csv_file:
                writer = csv.writer(csv_file)
                writer.writerow(columns)
                writer.writerows(rows)

        logging.info(f"Dumped SQLite responses table to CSV at {csv_path}")
