# PPO agent configuration
name: ppo

# Core training parameters
args:
  total_timesteps: 1000000
  learning_rate: 3e-4
  num_envs: 1
  num_steps: 2048
  anneal_lr: true
  # PPO algorithm parameters
  gamma: 0.8
  gae_lambda: 0.9
  num_minibatches: 32
  update_epochs: 10
  norm_adv: true
  clip_coef: 0.2
  clip_vloss: true
  ent_coef: 0.0
  vf_coef: 0.5
  max_grad_norm: 0.5
  target_kl: 0.1
  reward_scale: 1.0
  finite_horizon_gae: false
  normalize_obs: false
  # Network architecture
  hidden_size: 64
  actor_std_init: 0.01

# Training settings
torch_deterministic: true
cuda: true

# Logging settings
exp_name: "debug_ppo"
tb_log_dir: "runs"